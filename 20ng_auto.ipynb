{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__Import the required libraries \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and transforming it\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "vectorizer = TfidfVectorizer(stop_words='english',min_df=2)\n",
    "train = vectorizer.fit_transform(newsgroups.data)\n",
    "X_train = train.toarray()\n",
    "Y_train=newsgroups.target\n",
    "\n",
    "newsgroups2 = fetch_20newsgroups(subset='test')\n",
    "test = vectorizer.transform(newsgroups2.data)\n",
    "X_test = test.toarray()\n",
    "Y_test=newsgroups2.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#__all the constants\n",
    "learning_rate = 0.01\n",
    "num_steps = 30000\n",
    "batch_size = 256\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder for 20NG with K=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Autoencoder with K = 20\n",
    "input_dim = len(X_train[0])\n",
    "hidden_dim =20\n",
    "epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n",
    "        \n",
    "with tf.name_scope('encode'):\n",
    "    weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n",
    "    encoded = tf.nn.sigmoid(tf.matmul(x, weights) + biases)\n",
    "            \n",
    "with tf.name_scope('decode'):\n",
    "    weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n",
    "    decoded = tf.nn.sigmoid(tf.matmul(encoded, weights) + biases)\n",
    "            \n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(x,decoded))))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def get_batch(X, size):\n",
    "    a = np.random.choice(len(X), size, replace=False)\n",
    "    return X[a]\n",
    "\n",
    "def train(data):\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for i in range(epoch):\n",
    "            for j in range(np.shape(data)[0] // batch_size):\n",
    "                batch_data = get_batch(data, batch_size)\n",
    "                l, _ = sess.run([loss, train_op], feed_dict={x:batch_data})\n",
    "            print('epoch {0}: loss = {1}'.format(i+1, l))\n",
    "            saver.save(sess, './autoencoder.ckpt')\n",
    "        saver.save(sess, './autoencoder.ckpt')\n",
    "            \n",
    "def test(data):\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"./autoencoder.ckpt\")\n",
    "        hidden, reconstructed = sess.run([encoded, decoded], feed_dict={x:data})\n",
    "        return hidden, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 0.005129413679242134\n",
      "epoch 2: loss = 0.004350527189671993\n",
      "epoch 3: loss = 0.004268299322575331\n",
      "epoch 4: loss = 0.004255563020706177\n",
      "epoch 5: loss = 0.004227327182888985\n",
      "epoch 6: loss = 0.0042236484587192535\n",
      "epoch 7: loss = 0.004212360829114914\n",
      "epoch 8: loss = 0.004208816215395927\n",
      "epoch 9: loss = 0.00421184254810214\n",
      "epoch 10: loss = 0.00420741131529212\n",
      "INFO:tensorflow:Restoring parameters from ./autoencoder.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./autoencoder.ckpt\n"
     ]
    }
   ],
   "source": [
    "train(X_train)\n",
    "h,r=test(X_train)\n",
    "q,p=test(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      "ORIGINAL DATASET :\n",
      "    The training accuracy is  0.9777267102704614\n",
      "    The testing accuracy is  0.8327137546468402\n",
      "\n",
      "RECONSTRUCTED DATASET :\n",
      "    The training accuracy is  0.0839667668375464\n",
      "    The testing accuracy is  0.07713754646840149\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "\n",
    "print(\"\\nORIGINAL DATASET :\")\n",
    "loReg = LogisticRegression(penalty='l2',solver = 'lbfgs',multi_class='multinomial')\n",
    "loReg.fit(X_train,Y_train)\n",
    "score = loReg.score(X_train, Y_train)\n",
    "print(\" The training accuracy is \",score)\n",
    "score = loReg.score(X_test, Y_test)\n",
    "print(\" The testing accuracy is \",score)\n",
    "\n",
    "print(\"\\nRECONSTRUCTED DATASET :\")\n",
    "loReg = LogisticRegression(penalty='l2',solver = 'lbfgs',multi_class='multinomial')\n",
    "loReg.fit(r,Y_train)\n",
    "score = loReg.score(r, Y_train)\n",
    "print(\" The training accuracy is \",score)\n",
    "score = loReg.score(p, Y_test)\n",
    "print(\" The testing accuracy is \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder for 20NG with K=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 0.134831041097641\n",
      "epoch 2: loss = 0.10174984484910965\n",
      "epoch 3: loss = 0.08981727808713913\n",
      "epoch 4: loss = 0.0828646644949913\n",
      "epoch 5: loss = 0.0779142826795578\n",
      "epoch 6: loss = 0.07406099885702133\n",
      "epoch 7: loss = 0.06997630000114441\n",
      "epoch 8: loss = 0.06699789315462112\n",
      "epoch 9: loss = 0.0646982342004776\n",
      "epoch 10: loss = 0.062023572623729706\n",
      "INFO:tensorflow:Restoring parameters from ./autoencoder.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./autoencoder.ckpt\n",
      "Logistic Regression\n",
      "\n",
      "ORIGINAL DATASET :\n",
      "    The training accuracy is  0.9777267102704614\n",
      "    The testing accuracy is  0.8327137546468402\n",
      "\n",
      "RECONSTRUCTED DATASET :\n",
      "    The training accuracy is  0.1060632844263744\n",
      "    The testing accuracy is  0.06558682952734997\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(X_train[0])\n",
    "hidden_dim =200\n",
    "epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim])\n",
    "        \n",
    "with tf.name_scope('encode'):\n",
    "    weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n",
    "    encoded = tf.nn.sigmoid(tf.matmul(x, weights) + biases)\n",
    "            \n",
    "with tf.name_scope('decode'):\n",
    "    weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([input_dim]), name='biases')\n",
    "    decoded = tf.nn.sigmoid(tf.matmul(encoded, weights) + biases)\n",
    "            \n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(x,decoded))))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def get_batch(X, size):\n",
    "    a = np.random.choice(len(X), size, replace=False)\n",
    "    return X[a]\n",
    "\n",
    "def train(data):\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for i in range(epoch):\n",
    "            for j in range(np.shape(data)[0] // batch_size):\n",
    "                batch_data = get_batch(data, batch_size)\n",
    "                l, _ = sess.run([loss, train_op], feed_dict={x:batch_data})\n",
    "            print('epoch {0}: loss = {1}'.format(i+1, l))\n",
    "            saver.save(sess, './autoencoder.ckpt')\n",
    "        saver.save(sess, './autoencoder.ckpt')\n",
    "            \n",
    "def test(data):\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"./autoencoder.ckpt\")\n",
    "        hidden, reconstructed = sess.run([encoded, decoded], feed_dict={x:data})\n",
    "        return hidden, reconstructed\n",
    "\n",
    "train(X_train)\n",
    "h,r=test(X_train)\n",
    "q,p=test(X_test)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "\n",
    "print(\"\\nORIGINAL DATASET :\")\n",
    "loReg = LogisticRegression(penalty='l2',solver = 'lbfgs',multi_class='multinomial')\n",
    "loReg.fit(X_train,Y_train)\n",
    "score = loReg.score(X_train, Y_train)\n",
    "print(\" The training accuracy is \",score)\n",
    "score = loReg.score(X_test, Y_test)\n",
    "print(\" The testing accuracy is \",score)\n",
    "\n",
    "print(\"\\nRECONSTRUCTED DATASET :\")\n",
    "loReg = LogisticRegression(penalty='l2',solver = 'lbfgs',multi_class='multinomial')\n",
    "loReg.fit(r,Y_train)\n",
    "score = loReg.score(r, Y_train)\n",
    "print(\" The training accuracy is \",score)\n",
    "score = loReg.score(p, Y_test)\n",
    "print(\" The testing accuracy is \",score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
